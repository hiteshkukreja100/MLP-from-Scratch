# Multilayer Perceptron (MLP) Implementation on MNIST Dataset
Implemented a multilayer perceptron from scratch
# Overview
This project implements a Multilayer Perceptron (MLP) from scratch using Python and NumPy to classify handwritten digits from the MNIST dataset.

# Implementation Details
Initialization: Random initialization of weights and biases.

Activation Functions: ReLU for hidden layers and softmax for output layer.

Forward Propagation: Calculates activations through the network.

Backward Propagation: Computes gradients using backpropagation.

Training: Updates parameters using gradient descent.

# Key Learnings
Understanding of neural network fundamentals including forward and backward propagation.

Proficiency in NumPy for efficient matrix operations essential for neural networks.

Practical experience in debugging issues like vanishing gradients and initialization problems.

Importance of hyperparameter tuning for optimizing model performance.

# Results
Achieved an accuracy of 80%.

Iterative improvements in accuracy through debugging and hyperparameter adjustments.

# Conclusion
Implementing an MLP from scratch on MNIST provided valuable insights into neural network development, debugging techniques, and optimization strategies. Further experimentation and tuning are crucial for enhancing model performance and understanding complex datasets.

